#coding=utf8
#__author__='caichaopeng'

import os
import tensorflow as tf
import inference
import makeSample
import numpy as np

'''定义神经网络输入层图片的大小'''
image_size = 299
REGULARAZTION_RATE = 0.0001
TRAIN_STEPS = 50000
MODEL_SAVE_PATH = './model/'
MODEL_NAME = 'NN.ckpt'
LEARN_RATE_BASE = 0.0001
LEARN_RATE_DECAY = 0.99
sample_num = 3000
img_files_path = './trainFace'
classes = ['caichaopeng', 'jiangenyong','xiaweihai','huangcheng']  # 人为 设定4类
TFRecord_file_path = './tfrecords/face_train.tfrecords'
min_after_dequeue = 200
batch_size = 20
MOVING_AVERAGE_DECAY =0.99


def train(img_files_path, classes, TFRecord_file_path,image_size):
    makeSample.create_record(img_files_path, classes, TFRecord_file_path, image_size)
    img, label = makeSample.read_and_decode(TFRecord_file_path, image_size)
    '''将处理后的图像和标签数据通过tf.train.shuffle_batch整理成神经网络训练时需要的batch'''
    capacity = min_after_dequeue + 3 * batch_size
    image_batch,label_batch = tf.train.shuffle_batch(
        [img,label],batch_size = batch_size,
        capacity = capacity,min_after_dequeue = min_after_dequeue)
    d = image_batch
    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)
    print(image_batch.shape)
    logit = inference.inference(image_batch,True,regularizer)
    global_step = tf.Variable(0,trainable = False)

    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
    variable_averages_op = variable_averages.apply(tf.trainable_variables())

    cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit,labels =label_batch)
    cross_entropy_mean = tf.reduce_mean(cross_entropy)
    loss =cross_entropy_mean+tf.add_n(tf.get_collection('losses'))
    learning_rate=tf.train.exponential_decay(
        LEARN_RATE_BASE,
        global_step,
        sample_num/batch_size,
        LEARN_RATE_DECAY)
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)
    with tf.control_dependencies([train_step,variable_averages_op]):
        train_op = tf.no_op(name='train')

    saver = tf.train.Saver()
    with tf.Session() as sess:
        tf.global_variables_initializer().run()
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess = sess ,coord = coord)

        for i in range(TRAIN_STEPS):
            _, loss_value, step = sess.run([train_op, loss, global_step])

            if i % 10 == 0:
                # 输出当前的训练情况。这里只输出模型在当前batch下的损失函数大小。
                # 通过损失函数大小可以大概了解训练的情况。
                # 在验证数据集上的正确率信息会由一个单独的程序来生成
                print('After %d training step(s),loss on training batch is %f.' % (step, loss_value))
                # 保存当前的模型。注意这里给出了global_step参数，这样可以让每个被保存模型的文件名末尾加上训练的轮数，
                # 比如‘model.ckpt-1000’表示训练1000轮之后得到的模型
                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)

        coord.request_stop()
        coord.join(threads)





def main(argv = None):
    train(img_files_path, classes, TFRecord_file_path,image_size)

if __name__ == '__main__':
    tf.app.run()











